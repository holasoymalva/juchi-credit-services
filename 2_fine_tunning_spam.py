# -*- coding: utf-8 -*-
"""2. Fine Tunning - Spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pEK6XU7hg8L0UGNvEUPWbaLQ9Z33Ftgb

# Importing the data

* $0$ no spam
* $1$ spam
"""

import pandas as pd

path = "/content/drive/MyDrive/Colab Notebooks/HackMexico/finalData.csv"
myData=pd.read_csv(path)[["cleanMessage", "Spam/Ham"]]
myData.head()

myData = myData[myData.notnull()]
myData=myData.dropna()

myData.info()

"""# Splitting the data"""

from sklearn.model_selection import train_test_split
trainData, testData=train_test_split(myData,
                                     test_size=0.2,
                                     stratify=myData["Spam/Ham"],
                                     random_state=42)

len(trainData), len(testData)

"""# Creating the `DatasetDict`"""

!pip install datasets
from datasets import Dataset, DatasetDict

# Convierte los DataFrames a Datasets:
train = Dataset.from_pandas(trainData).remove_columns("__index_level_0__")
test = Dataset.from_pandas(testData).remove_columns("__index_level_0__")

# Creating the DatasetDict
realDataset = DatasetDict({
    'train': train,
    'test': test
})

realDataset

"""*Visualizing an element*"""

realDataset["train"][3]

print("Done")

"""# Tokenizing

*Importing the tokenizer*
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

"""*Tokenizing function*"""

tokenize_function = lambda x: tokenizer(x["cleanMessage"],
                                       padding="max_length",
                                       truncation=True)

tokenized_datasets = realDataset.map(tokenize_function, batched=True)
tokenized_datasets

"""*Optional* **Small datasets**"""

len(tokenized_datasets["test"])

ltrain=int(len(tokenized_datasets["train"])*0.8)
ltest=int(len(tokenized_datasets["test"])*0.2)

small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(ltrain))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(ltest))

"""# Train

* With native `PyTorch`

*Remove the text column because the model does not accept raw text as an input*
"""

tokenized_datasets = tokenized_datasets.remove_columns(["cleanMessage"])

"""*Renaming the column* `Spam/Ham` *as* `labels`"""

tokenized_datasets = tokenized_datasets.rename_column("Spam/Ham", "labels")

"""*Return PyTorch tensors instead of lists*"""

tokenized_datasets.set_format("torch")

"""**Creating the** `DataLoader`"""

from torch.utils.data import DataLoader

train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=8)
eval_dataloader = DataLoader(tokenized_datasets["test"], batch_size=8)

"""**Loading the model with the number of expected labels**"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=2)

"""**Optimizer and learning rate scheduler**

* `AdamW` optimizer
"""

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

"""*Default learning rate scheduler from* `Trainer`"""

from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

"""`CUDA` or `CPU`"""

import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
device

model.to(device)

"""# Training loop"""

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

"""# Evaluation

*Accumulate all the batches with* `add_batch` *and calculate the metric at the very end.*
"""

!pip install evaluate
import evaluate

metric = evaluate.load("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()

# Supongamos que 'model' es tu modelo afinado
torch.save(model.state_dict(), "/content/drive/MyDrive/Colab Notebooks/HackMexico/modelo.pth")

"""# Importing the model"""

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=2)
model.load_state_dict(torch.load("/content/drive/MyDrive/Colab Notebooks/HackMexico/modelo.pth"))
model.eval()